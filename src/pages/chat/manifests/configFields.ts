import { ConfigFields } from "@/schema/template-chat-settings-types";

export const configFields: ConfigFields = [
  {
    name: "temperature",
    type: "stepbutton_slider",
    title: "Temperature",
    description: "Controls the randomness of the output. Lower values make the output more deterministic, higher values increase randomness.",
    min: 0,
    max: 5,
    step: 0.25,
  },
  {
    name: "top_p",
    type: "stepbutton",
    title: "Top P",
    description: "Nucleus sampling: Selects the smallest set of tokens whose cumulative probability exceeds the threshold p. 1 = disabled.",
    min: 0,
    max: 1,
    step: 0.25,
    default: 1,
  },
  {
    name: "top_k",
    type: "stepbutton",
    title: "Top K",
    description: "Selects the next token only from the top k most likely tokens. 0 = disabled.",
    min: 0,
    max: 200,
    step: 10,
    default: 50,
  },
  {
    name: "min_p",
    type: "stepbutton_slider",
    title: "Min P",
    description:
      "Minimum P sampling: Sets a minimum probability threshold relative to the most likely token's probability. Tokens below this are excluded.",
    min: 0,
    max: 1,
    step: 0.25,
    default: 0,
  },
  {
    name: "top_a",
    type: "stepbutton",
    title: "Top A",
    description:
      "Top A sampling: Filters tokens based on their probability relative to the top token's probability squared. Used in some specific model architectures.",
    min: 0,
    max: 1,
    step: 0.25,
    default: 0,
  },
  {
    name: "nsigma",
    type: "stepbutton",
    title: "Sigma Sampling (NSigma)",
    description:
      "Scales logits by their standard deviation. Higher values flatten the distribution (more random), lower values sharpen it. 1 = no effect.",
    min: 0,
    max: 10,
    step: 0.15,
    default: 1.67,
  },
  {
    name: "frequency_penalty",
    type: "stepbutton",
    title: "Frequency Penalty",
    description:
      "Penalizes new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
    min: -2,
    max: 2,
    step: 0.25,
    default: 0,
  },
  {
    name: "presence_penalty",
    type: "stepbutton",
    title: "Presence Penalty",
    description:
      "Penalizes new tokens based on whether they have appeared in the text so far, increasing the model's likelihood to talk about new topics.",
    min: -2,
    max: 2,
    step: 0.25,
    default: 0,
  },
  {
    name: "repetition_penalty",
    type: "stepbutton",
    title: "Repetition Penalty",
    description: "Penalizes the probability of repeating tokens that have recently appeared. 1 = no penalty.",
    min: 1,
    max: 3,
    step: 0.25,
    default: 1,
  },
  {
    name: "seed",
    type: "random_number",
    title: "Seed",
    description: "Seed for the random number generator to ensure reproducibility. -1 uses a random seed.",
    min: -1,
    max: 1000000000,
    default: -1,
  },
  {
    name: "smoothing_sampling",
    type: "section",
    title: "Smooth Sampling",
    description: "Settings for Quadratic Sampling which modifies the logits distribution.",
    fields: [
      {
        name: "smoothing_factor",
        type: "stepbutton",
        title: "Smooth Factor",
        description: "Controls the intensity of the quadratic smoothing. >1 peaks the distribution, <1 flattens it.",
        min: 0,
        max: 10,
        step: 0.15,
        default: 0,
      },
      {
        name: "smoothing_curve",
        type: "stepbutton",
        title: "Smooth Curve",
        description: "Determines the curve shape used for quadratic smoothing.",
        min: 1,
        max: 10,
        step: 0.5,
        default: 1,
      },
    ],
  },
  {
    name: "dry",
    type: "section",
    title: "Dry Repetition Penalty",
    description: "Dynamic Repetition Yield (DRY) settings for advanced control over token repetition.",
    fields: [
      {
        name: "dry_multiplier",
        type: "stepbutton_slider",
        title: "Multiplier",
        description: "Scaling factor for the DRY penalty calculation.",
        min: 0,
        max: 5,
        default: 0,
        step: 0.1,
      },
      {
        name: "dry_base",
        type: "stepbutton",
        title: "Base",
        description: "Base value used in the exponential calculation of the DRY penalty.",
        min: 1,
        max: 4,
        default: 1.75,
        step: 0.15,
      },
      {
        name: "dry_allowed_length",
        type: "stepbutton",
        title: "Allowed Length",
        description: "Maximum allowed length of repeated sequences before DRY penalty applies.",
        min: 0,
        max: 20,
        default: 0,
        step: 1,
      },
      {
        name: "dry_penalty_last_n",
        type: "stepbutton",
        title: "Penalty Range",
        description: "Number of recent tokens considered for DRY penalty calculation.",
        min: 0,
        max: 2048,
        default: 512,
        step: 64,
      },
      {
        name: "dry_sequence_breakers",
        type: "string_array",
        title: "Sequence Breakers",
        description: "Characters or strings that reset the sequence count for DRY penalty.",
        default: ["\\n", ":", '"', "*"],
      },
    ],
  },
  {
    name: "xtc",
    type: "section",
    title: "Exclude Top Choices (XTC)",
    description: "Exclude Top Choices (XTC) settings for preventing the model from selecting the most probable tokens.",
    fields: [
      {
        name: "xtc_threshold",
        type: "stepbutton_slider",
        title: "Threshold",
        description: "Probability threshold. Tokens above this cumulative probability might be excluded based on xtc_probability.",
        min: 0,
        max: 0.5,
        default: 0.1,
        step: 0.05,
      },
      {
        name: "xtc_probability",
        type: "stepbutton_slider",
        title: "Probability",
        description: "Probability of applying XTC filtering at each sampling step.",
        min: 0,
        max: 1,
        default: 0,
        step: 0.05,
      },
    ],
  },
  {
    name: "dynamic_temperature",
    type: "section",
    title: "Dynamic Temperature",
    description: "Adjusts sampling temperature dynamically based on token probability/entropy to balance creativity and coherence.",
    fields: [
      {
        name: "dynatemp_low",
        type: "stepbutton",
        title: "Low",
        description: "Minimum temperature applied when model confidence is high (low entropy).",
        min: 0,
        max: 5,
        default: 0.5,
        step: 0.15,
      },
      {
        name: "dynatemp_high",
        type: "stepbutton",
        title: "High",
        description: "Maximum temperature applied when model confidence is low (high entropy), allowing more randomness.",
        min: 0,
        max: 5,
        default: 1.5,
        step: 0.15,
      },
      {
        name: "dynatemp_exponent",
        type: "stepbutton",
        title: "Exponent",
        description: "Controls the sensitivity/steepness of temperature scaling between low and high based on entropy.",
        min: 0,
        max: 10,
        default: 1,
        step: 0.5,
      },
    ],
  },
  {
    name: "sampling_order",
    type: "drag_array",
    title: "Sampling Order",
    description: "Defines the order in which different sampling methods are applied.",
    default: ["dry", "xtc", "min_p", "top_p", "top_k", "temperature"],
    options: ["dry", "xtc", "min_p", "top_p", "top_k", "temperature"],
    readOnly: true,
  },
  {
    name: "reasoning",
    type: "section",
    title: "Reasoning",
    description: "Settings related to enabling specific reasoning or guided generation modes in certain models.",
    fields: [
      {
        name: "reasoning_budget",
        type: "stepbutton",
        title: "Reasoning Budget",
        description: "Parameter influencing reasoning capabilities, potentially related to context or token allocation (model specific).",
        min: 0,
        max: 63999,
        step: 1024,
        default: 1024,
      },
      {
        name: "reasoning_temperature",
        type: "stepbutton_slider",
        title: "Reasoning Effort (3=High)",
        description: "Controls the intensity or effort applied during the reasoning/guided generation phase (model specific).",
        min: 1,
        max: 3,
        step: 1,
        default: 1,
      },
    ],
  },
  {
    name: "prompt_cache",
    type: "section",
    title: "Prompt Cache",
    description: "Settings for caching prompt computations to speed up generation, primarily for specific APIs/models.",
    fields: [
      {
        name: "chat_depth",
        type: "stepbutton",
        title: "Chat Depth",
        description: "Number of recent conversation turns to include in the prompt cache.",
        min: 0,
        max: 50,
        step: 1,
        default: 0,
      },
    ],
  },
];
